# COMPANY MEMORY

<!--
Append reusable org-wide learnings that could help colleagues too.
Do NOT dump raw conversation.
Avoid secrets or sensitive information.
-->

- Data pipeline involves structured sequence of processes for data ingestion, transformation, and delivery
- Sources include databases, APIs, logs, IoT devices (structured, unstructured, semi-structured)
- Processing steps involve ETL/ELT, validation, or real-time analytics (tools: Apache Spark, Flink, custom scripts)
- Destinations ("sinks") include data lakes, warehouses, databases (e.g., S3, Delta Lake, Snowflake)
- Cloud-based pipelines emphasize scalability, flexibility, and agility
- Document references Databricks as a
- Data pipelines involve ETL (Extract, Transform, Load) processes
- Pipelines handle structured, unstructured, and semi-structured data formats
- Key frameworks for scalability include Spark
- Cloud-based pipelines (e.g., Databricks) are preferred for scalability and flexibility
- Pipelines require modular design, reliability, and agility for modern data workflows
- Destinations for processed data include data lakes, data warehouses, and analytics engines
